from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.pydantic_v1 import BaseModel, Field
from pydantic import BaseModel, Field
from langchain_core.messages import SystemMessage, HumanMessage

class GradeDocuments(BaseModel):
    """Stores the binary score generated by the LLM for relevance check on retrieved documents"""

    binary_score: str = Field(description="Binary score if document is relevant to the question, 'Yes' if they are relevant else 'No'")


#  If the document contains semantic meaning or is relevant to the given question,
#   grade it as relevant and score it as 'Yes', otherwise 'No'.
#   'Yes' means that the document is relevant and contains semantic meaning to the question.

grader_system_message = """
  You are an expert Grader assessing relevance of a retrieved document to a user question.
  If the document contains keywords related to the user question, 
  grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n
  Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.
"""

grader_prompt = ChatPromptTemplate.from_messages(
    [
        SystemMessage(content= grader_system_message),
        HumanMessage(content="Retrieved document: \n\n {document} \n\n User Question: {question}")
    ]
)


class GradeHallucination(BaseModel):   
   """Binary score for hallucination present in generated answer"""
   binary_score: str = Field(
       description="Answer is grounded in the facts, 'yes' or 'no'"
       )


hallucination_system_message = """
You are an expert Grader assessing whether LLM generated output is grounded in or
supported by the set of retrieved facts. \n
Give a binary score of 'yes' or 'no'. 'Yes' means that the answer is grounded in or supported by the set of facts.
"""

hallucination_prompt = ChatPromptTemplate.from_messages(
    [
        SystemMessage(content=hallucination_system_message),
        HumanMessage(content="Set of facts: \n\n {documents} \n\n LLM generated output: {generations}")
    ]
)

class GradeAnswer(BaseModel):
    """Binary score if generated LLM answer addresses the question"""

    binary_score: str = Field(
        description="Answer addresses the question, 'yes' or 'no'"
    )

answer_grader_system_message = """
You are a grader assessing whether generated LLM answer addressess / reolves a question \n
Give a binary score 'yes' or 'no'. 'Yes' means that the answer resolves the question
"""

answer_grader_prompt = ChatPromptTemplate.from_messages(
    [
        SystemMessage(content= answer_grader_system_message),
        HumanMessage(content="User question: \n\n {question} \n\n LLM answer: {generation}")
    ]
)


